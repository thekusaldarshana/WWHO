# Usage Guide

This guide covers the main workflows for WWHO: installation, training a new vocabulary, running tests, and exporting the tokenizer for use.

## Installation

Ensure you have Python 3.9+ installed. Clone the repository and install the dependencies:

```bash
git clone https://github.com/your-username/wwho.git
cd wwho
pip install -r requirements.txt
```

You will need `tiktoken` and `rich` for the CLI interface.

## Quick Start: The Orchestrator

The easiest way to interact with WWHO is via the `orchestrator.py` script. It provides an interactive menu for all major tasks.

```bash
python orchestrator.py
```

This will launch a menu:

1.  **Train:** Run `gpe_trainer.py` on your corpus.
2.  **Evaluate:** Run benchmarks against other tokenizers.
3.  **Export:** Generate Hugging Face compatible tokenizer files.
4.  **Run Tests:** Execute the full test suite.
5.  **Schema Check:** Validate your JSON schemas.
6.  **Full Pipeline:** Run all steps in sequence.

## Manual Usage

### 1. Training a Vocabulary

To train a new SGPE vocabulary from scratch, use `gpe_trainer.py`.

**Arguments:**
*   `--train_file`: Path to your training corpus (JSONL format, one JSON object per line with a `"text"` field).
*   `--vocab_size`: Target vocabulary size (e.g., `30000`).
*   `--min_freq`: Minimum frequency for a token to be considered for merging.
*   `--prune_freq`: Frequency threshold for pruning rare syllables before BPE starts.
*   `--output_dir`: Directory to save `vocab.json`.
*   `--script_mode`: `sinhala`, `devanagari`, or `mixed` (default).

```bash
python gpe_trainer.py \
    --train_file dataset/mixed_train.jsonl \
    --vocab_size 30000 \
    --min_freq 2 \
    --output_dir output \
    --script_mode mixed
```

### 2. Exporting Tokenizers

After training, you need to export the vocabulary into a usable format.

**Arguments:**
*   `--vocab`: Path to the trained `vocab.json`.
*   `--out_dir`: Output directory for `tokenizer.json` and `tokenizer_meta.json`.
*   `--tiktoken_model`: The tiktoken model to use for offset calculation (default: `o200k_base`).

```bash
python export.py \
    --vocab output/vocab.json \
    --out_dir exports \
    --tiktoken_model o200k_base
```

This generates:
*   `exports/tokenizer.json`: Standard HF tokenizer for the SGPE part.
*   `exports/tokenizer_meta.json`: The meta-vocabulary definition (Tiktoken + Offset SGPE).

### 3. Running Tests

WWHO comes with a rigorous test suite in `tests/battle_v2.py`.

**Arguments:**
*   `--vocab_file`: Path to your exported `vocab.json` (or the one generated by `gpe_trainer.py`).
*   `--test_file`: Path to a test corpus (JSONL).
*   `--only`: Run specific test batteries (e.g., `complexity`, `glitched`, `roundtrip`).

```bash
python tests/battle_v2.py \
    --vocab_file output/vocab.json \
    --test_file dataset/mixed_test.jsonl \
    --only complexity roundtrip
```

### 4. Using the Tokenizer in Python

To use the trained tokenizer in your own scripts:

```python
from encoder import WWHOMetaEncoder

# Initialize with the path to your vocab.json
# It will automatically detect the meta-config if present, or fallback to defaults
encoder = WWHOMetaEncoder("exports/vocab.json")

text = "Hello ලංකාව!"

# Encode to unified IDs
ids = encoder.encode(text)
print("IDs:", ids)
# Output might look like: [15339, 220, 200451, 200102, 0]
# (Tiktoken IDs mixed with high SGPE IDs)

# Decode back to string
decoded_text = encoder.decode(ids)
print("Decoded:", decoded_text)
assert text == decoded_text
```

## Data Format

Both training and testing scripts expect data in **JSONL** (JSON Lines) format. Each line must be a valid JSON object containing a `"text"` key.

```json
{"text": "This is an example sentence."}
{"text": "මෙය උදාහරණ වාක්‍යයකි."}
```
